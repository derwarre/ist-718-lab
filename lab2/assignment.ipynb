{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import zipcodes\n",
    "from math import sqrt\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read zillow data\n",
    "with open('../data/Zip_Zhvi_SingleFamilyResidence.csv', 'rb') as f:\n",
    "    df_zillow = pd.read_csv(f).fillna(0)\n",
    "\n",
    "# read unemployment data\n",
    "with open('../data/laucnty17.xlsx', 'rb') as f:\n",
    "    df_labor = pd.read_excel(f, skiprows=range(1, 6)).fillna(0)\n",
    "\n",
    "# read crime data\n",
    "with open('../data/district-of-columbia.xls', 'rb') as f:\n",
    "    df_crime_dc = pd.read_excel(f, skiprows=range(1, 2)).fillna(0)\n",
    "\n",
    "with open('../data/maryland.xls', 'rb') as f:\n",
    "    df_crime_md = pd.read_excel(f, skiprows=range(1, 5)).fillna(0)\n",
    "\n",
    "with open('../data/massachusetts.xls', 'rb') as f:\n",
    "    df_crime_ma = pd.read_excel(f, skiprows=range(1, 5)).fillna(0)\n",
    "\n",
    "with open('../data/virginia.xls', 'rb') as f:\n",
    "    df_crime_va = pd.read_excel(f, skiprows=range(1, 5)).fillna(0)\n",
    "\n",
    "with open('../data/new-hampshire.xls', 'rb') as f:\n",
    "    df_crime_nh = pd.read_excel(f, skiprows=range(1, 5)).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crime columns\n",
    "#dc_col_list = df_crime_dc.iloc[:, 3:12].columns.tolist()\n",
    "md_col_list = df_crime_md.iloc[:, 3:12].columns.tolist()\n",
    "ma_col_list = df_crime_ma.iloc[:, 3:12].columns.tolist()\n",
    "va_col_list = df_crime_va.iloc[:, 3:12].columns.tolist()\n",
    "nh_col_list = df_crime_nh.iloc[:, 3:12].columns.tolist()\n",
    "\n",
    "# crime sum: simplified by summing all offense types\n",
    "#dc_sum = df_crime_dc[dc_col_list].sum(axis=1)\n",
    "md_sum = df_crime_md[md_col_list].sum(axis=1)\n",
    "ma_sum = df_crime_ma[ma_col_list].sum(axis=1)\n",
    "va_sum = df_crime_va[va_col_list].sum(axis=1)\n",
    "nh_sum = df_crime_nh[nh_col_list].sum(axis=1)\n",
    "\n",
    "# crime ratio: offense sum / population\n",
    "df_md = df_crime_md\n",
    "df_md['City'] = df_crime_md.iloc[:, 0]\n",
    "df_md['State'] = 'md'\n",
    "df_md['Population'] = df_crime_md.iloc[:, 1]\n",
    "df_md['CrimeRatio'] = md_sum/df_crime_md.iloc[:, 1]\n",
    "df_md = df_crime_md[['City', 'Population', 'CrimeRatio', 'State']]\n",
    "\n",
    "df_ma = df_crime_ma\n",
    "df_ma['City'] = df_crime_ma.iloc[:, 0]\n",
    "df_ma['State'] = 'ma'\n",
    "df_ma['Population'] = df_crime_ma.iloc[:, 1]\n",
    "df_ma['CrimeRatio'] = ma_sum/df_crime_ma.iloc[:, 1]\n",
    "df_ma = df_crime_ma[['City', 'Population', 'CrimeRatio', 'State']]\n",
    "\n",
    "df_va = df_crime_va\n",
    "df_va['City'] = df_crime_va.iloc[:, 0]\n",
    "df_va['State'] = 'va'\n",
    "df_va['Population'] = df_crime_va.iloc[:, 1]\n",
    "df_va['CrimeRatio'] = va_sum/df_crime_va.iloc[:, 1]\n",
    "df_va = df_crime_va[['City', 'Population', 'CrimeRatio', 'State']]\n",
    "\n",
    "df_nh = df_crime_nh\n",
    "df_nh['City'] = df_crime_nh.iloc[:, 0]\n",
    "df_nh['State'] = 'nh'\n",
    "df_nh['Population'] = df_crime_nh.iloc[:, 1]\n",
    "df_nh['CrimeRatio'] = nh_sum/df_crime_nh.iloc[:, 1]\n",
    "df_nh = df_crime_nh[['City', 'Population', 'CrimeRatio', 'State']]\n",
    "\n",
    "# combine dataframe\n",
    "df_crime = pd.concat([df_md, df_ma, df_va, df_nh])\n",
    "\n",
    "# remove unneeded column\n",
    "df_crime.drop(['Population'], axis=1, inplace=True)\n",
    "\n",
    "# convert to lowercase\n",
    "df_crime[['City', 'State']] = df_crime[['City', 'State']].apply(\n",
    "  lambda x: x.astype(str).str.lower().map(lambda x: x.strip())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restructure labor columns\n",
    "df_labor = pd.DataFrame(df_labor.iloc[:, [3,9]])\n",
    "df_labor.columns = ['Location', 'Unemployment']\n",
    "df_labor[['CountyName', 'State']] = df_labor['Location'].str.split(',', expand=True)\n",
    "df_labor.drop(['Location'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert non-timeseries\n",
    "df_zillow[['City', 'State', 'Metro', 'CountyName']] = df_zillow[['City', 'State', 'Metro', 'CountyName']].astype(str)\n",
    "df_zillow[['RegionID', 'RegionName', 'SizeRank']] = df_zillow[['RegionID', 'RegionName', 'SizeRank']].astype(int)\n",
    "\n",
    "# convert to lowercase\n",
    "df_zillow[['City', 'State', 'Metro', 'CountyName']] = df_zillow[['City', 'State', 'Metro', 'CountyName']].apply(\n",
    "  lambda x: x.astype(str).str.lower().map(lambda x: x.strip())\n",
    ")\n",
    "df_labor[['CountyName', 'State']] = df_labor[['CountyName', 'State']].apply(\n",
    "  lambda x: x.astype(str).str.lower().map(lambda x: x.strip())\n",
    ")\n",
    "\n",
    "# remove redundant suffix\n",
    "df_labor[['CountyName']] = df_labor['CountyName'].str.rstrip('county').map(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arkansas metro areas\n",
    "hot_springs = df_zillow.loc[(df_zillow['Metro'] == 'hot springs') & (df_zillow['State'] == 'ar')]\n",
    "little_rock = df_zillow.loc[(df_zillow['Metro'] == 'little rock') & (df_zillow['State'] == 'ar')]\n",
    "fayetteville = df_zillow.loc[(df_zillow['Metro'] == 'fayetteville') & (df_zillow['State'] == 'ar')]\n",
    "searcy = df_zillow.loc[(df_zillow['Metro'] == 'searcy') & (df_zillow['State'] == 'ar')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes\n",
    "df = pd.merge(df_zillow, df_labor, on=['CountyName', 'State'])\n",
    "df = pd.merge(df, df_crime, on=['City', 'State'])\n",
    "\n",
    "# timeseries data\n",
    "ts_start = df.columns.get_loc('1996-04') + 1\n",
    "ts_end = df.columns.get_loc('2017-09')\n",
    "date_columns = df.iloc[:, ts_start:ts_end].columns.tolist()\n",
    "\n",
    "# ensure integer timeseries\n",
    "df[date_columns] = df[date_columns].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeseries plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(hot_springs[date_columns].mean(), linestyle='solid')\n",
    "ax.plot(little_rock[date_columns].mean(), linestyle='solid')\n",
    "ax.plot(fayetteville[date_columns].mean(), linestyle='solid')\n",
    "ax.plot(searcy[date_columns].mean(), linestyle='solid')\n",
    "\n",
    "# decrease ticks\n",
    "xmin, xmax = ax.get_xlim()\n",
    "ax.set_xticks(np.round(np.linspace(xmin, xmax, 23), 2))\n",
    "\n",
    "# rotate ticks + show legend\n",
    "plt.xticks(rotation=90)\n",
    "plt.gca().legend(('hot_springs', 'little_rock', 'fayetteville', 'searcy'))\n",
    "\n",
    "# show overall plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with unemployment >= 3.5%\n",
    "df = df[df.Unemployment < 3.5]\n",
    "\n",
    "# remove rows with crime >= 3%\n",
    "df = df[df.CrimeRatio < 0.03]\n",
    "\n",
    "# remove redundant columns\n",
    "df.drop(['Unemployment'], axis=1, inplace=True)\n",
    "df.drop(['CrimeRatio'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter specific states\n",
    "df = df.loc[df['State'].isin(['md','va', 'nh', 'ma', 'dc'])]\n",
    "\n",
    "# remove specific cities\n",
    "df = df.loc[-((df['Metro'] == 'baltimore') & (df['State'] == 'md'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train: collapse column by median\n",
    "train_start = df.columns.get_loc('1997-01')\n",
    "train_stop = df.columns.get_loc('2017-01')\n",
    "test_stop = df.columns.get_loc('2017-09')\n",
    "train_columns = df.iloc[:, train_start:train_stop].columns.tolist()\n",
    "test_columns = df.iloc[:, (train_stop + 1):test_stop].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with 0's beginning (1997-01) with trainset\n",
    "date_columns = df.iloc[:, train_start:test_stop].columns.tolist()\n",
    "\n",
    "df[date_columns] = df[date_columns].replace(0, np.nan)\n",
    "df[date_columns] = df[date_columns].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# transpose dataframe: left column data, right column value\n",
    "#\n",
    "#     date1  val1\n",
    "#     date2  val2\n",
    "#      ...   ...\n",
    "#     daten  valn\n",
    "#\n",
    "df_train = df[train_columns].median().T\n",
    "df_test = df[test_columns].median().T\n",
    "\n",
    "#\n",
    "# build arima model:\n",
    "#\n",
    "#     AR: autoregression, uses observations from previous time steps as input to\n",
    "#         a regression equation to predict the value at the next time step.\n",
    "#\n",
    "#     I: integrated, use of differencing of raw observations, or subtracting an\n",
    "#         observation from previous time step. The goal is to attain a time\n",
    "#         series that is stationary.\n",
    "#\n",
    "#     MA: moving average, uses the dependency between an observation and a residual\n",
    "#         error from a moving average model applied to lagged observations.\n",
    "#\n",
    "model = ARIMA(df_train, order=(5,1,0))\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residual errors\n",
    "def residuals_plot(model_fit):\n",
    "    residuals = DataFrame(model_fit.resid)\n",
    "    residuals.plot()\n",
    "    plt.show()\n",
    "\n",
    "    # plot kernel density estimation\n",
    "    residuals.plot(kind='kde')\n",
    "    plt.show()\n",
    "\n",
    "residuals_plot(model_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive statistics on residual\n",
    "print(residuals.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# rolling prediction (verify model): month +2, since the train\n",
    "#     includes 2017-01.\n",
    "#\n",
    "# Note: rolling prediction is required since there is an implicit\n",
    "#       dependence on observations in prior time steps inheritted\n",
    "#       when autogressive (AR) model was defined.\n",
    "#\n",
    "history = [x for x in df_train]\n",
    "predictions = list()\n",
    "iterations = (12-len(df_test)) + 18\n",
    "\n",
    "for t in range(iterations):\n",
    "    model = ARIMA(history, order=(5,1,0))\n",
    "    model_fit = model.fit(disp=0)\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    predictions.append(yhat)\n",
    "\n",
    "    if t > 10:\n",
    "        year = 2018\n",
    "        month = (t+2) % 12\n",
    "        if month == 0:\n",
    "            month = 12\n",
    "    else:\n",
    "        year = 2017\n",
    "        month = t+2\n",
    "        if month == 0:\n",
    "            month = 12\n",
    "\n",
    "    print('\\n===============================================')\n",
    "    print('date: {}-{:01d}'.format(year, month))\n",
    "    print('-----------------------------------------------')\n",
    "\n",
    "    #\n",
    "    # observation: if current value doesn't exist from test, append current\n",
    "    #     predition, to ensure successive rolling prediction computed.\n",
    "    #\n",
    "    try:\n",
    "        obs = df_test[t]\n",
    "        print('predicted={:03f}, expected={:03f}'.format(float(yhat), obs))\n",
    "        print('prediction difference: {:03f}'.format(abs(1-float(yhat)/obs)))\n",
    "        print('\\n===============================================')\n",
    "        error = mean_squared_error(df_test, predictions)\n",
    "        print('Test MSE: {:03f}\\n\\n'.format(error))\n",
    "    except:\n",
    "        obs = yhat\n",
    "        print('predicted={:03f}'.format(float(yhat)))\n",
    "\n",
    "    history.append(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rolling prediction\n",
    "def rolling_plot(data, predictions):\n",
    "    plt.plot(data)\n",
    "    plt.plot(predictions, color='red')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "rolling_plot(df_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stationarity test\n",
    "def difference(dataset, interval):\n",
    "    diff = list()\n",
    "    for i in range(1, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return pd.Series(diff)\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "    return yhat + history[-interval]\n",
    "\n",
    "def compute_arima(\n",
    "    data=df_train,\n",
    "    p=5,\n",
    "    q=0,\n",
    "    d=0,\n",
    "    delta=(12-len(df_test)) + 18,\n",
    "    alpha=0.05,\n",
    "    residuals_plot=False\n",
    "):\n",
    "    #\n",
    "    # Note: rolling prediction is required since there is an implicit\n",
    "    #       dependence on observations in prior time steps inheritted\n",
    "    #       when autogressive (AR) model was defined.\n",
    "    #\n",
    "    history = [x for x in data]\n",
    "    predictions = list()\n",
    "    model_fit = False\n",
    "\n",
    "    # generate model\n",
    "    try:\n",
    "        model = ARIMA(difference(history, delta), order=(p,q,d))\n",
    "        model_fit = model.fit(disp=0)\n",
    "        print('standard fit used')\n",
    "    except Exception as e:\n",
    "        print('stationary differences will be used')\n",
    "        print('original error: {}'.format(e))\n",
    "\n",
    "    # significant stationarity: use any that works\n",
    "    if not model_fit:\n",
    "        #\n",
    "        # determine stationarity value: differencing handled with supplied data,\n",
    "        #     as an indirect solution, since statsmodel not allow d > 2.\n",
    "        #\n",
    "        # @delta, autoregressive factor.\n",
    "        #\n",
    "        for delta in range(10):\n",
    "            stationary = difference(history, delta)\n",
    "            stationary.index = history[1:]\n",
    "            result = adfuller(stationary)\n",
    "            print('stationary fit: {}, p: {}'.format(delta, result[1]))\n",
    "\n",
    "            #\n",
    "            # generate model: use high (10) autoregression, since data is not\n",
    "            #     seasonal. Therefore, using previous values is conservative.\n",
    "            #\n",
    "            if (result[1] <= 0.05):\n",
    "                try:\n",
    "                    model = ARIMA(stationary, order=(p,q,d))\n",
    "                    model_fit = model.fit(disp=0)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print('bad condition {}: stationarity not adequate'.format(delta))\n",
    "                    print('original error: {}'.format(e))\n",
    "                    continue\n",
    "\n",
    "    #\n",
    "    # generate forecast: an inverse difference is needed to reverse the earlier\n",
    "    #     difference model scaling.\n",
    "    #\n",
    "    if model_fit:\n",
    "        output = model_fit.forecast(steps=delta, alpha=alpha)[0]\n",
    "        if residuals_plot:\n",
    "            residuals_plot(model_fit)\n",
    "\n",
    "    for yhat in output:\n",
    "        inverted = inverse_difference(history, yhat, interval=delta)\n",
    "        history.append(inverted)\n",
    "        predictions.append(inverted)\n",
    "\n",
    "    print('predictions: {}'.format(predictions))\n",
    "\n",
    "    return(predictions)\n",
    "\n",
    "def get_zipcode(city, state):\n",
    "    result = zipcodes.filter_by(\n",
    "        zipcodes.list_all(),\n",
    "        active=True,\n",
    "        city=city,\n",
    "        state=state\n",
    "    )\n",
    "\n",
    "    if result and result[0] and result[0]['zip_code']:\n",
    "        return(result[0]['zip_code'])\n",
    "    else:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add zipcode column\n",
    "df['zip_code'] = df[['City', 'State']].apply(\n",
    "    lambda x: get_zipcode(\n",
    "        x['City'].upper(),\n",
    "        x['State'].upper()\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by zipcode\n",
    "df_zipcode = df.groupby('zip_code').agg(np.median).dropna().T\n",
    "\n",
    "#\n",
    "# remove columns: column 0 indicates an NaN column\n",
    "#\n",
    "df_zipcode_clean = df_zipcode.drop([\n",
    "    'RegionName',\n",
    "    'RegionID',\n",
    "    'SizeRank'\n",
    "], axis=0)\n",
    "df_zipcode_clean = df_zipcode_clean.drop([0], axis=1)\n",
    "\n",
    "df_zipcode_clean.plot(legend=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate columns\n",
    "results = []\n",
    "for column in df_zipcode_clean.T[train_columns].T:\n",
    "    predictions = compute_arima(df_zipcode_clean[column], q=1)\n",
    "    results.append({\n",
    "        'zip_code': df_zipcode_clean[column].name,\n",
    "        'predictions': predictions\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local variables\n",
    "agg = []\n",
    "\n",
    "# sum difference squared\n",
    "for zipcode in results:\n",
    "    z = zipcode['zip_code']\n",
    "    actual = df_zipcode_clean.T[test_columns].T[z]\n",
    "    print(actual)\n",
    "    predicted = zipcode['predictions']\n",
    "    print('=================================================')\n",
    "    print(z)\n",
    "    print('=================================================')\n",
    "\n",
    "    # compare differences\n",
    "    sum = 0\n",
    "    for act, pred in zip(actual, predicted[:len(test_columns)]):\n",
    "        sum += sqrt((act - pred)**2)\n",
    "        print('predicted={:03f}, expected={:03f}'.format(float(pred), act))\n",
    "        print('prediction difference: {:03f}'.format(abs(1-float(pred)/act)))\n",
    "        print('-------------------------------------------------')\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    # aggregate data\n",
    "    agg.append({\n",
    "        'zipcode': zipcode['zip_code'],\n",
    "        'difference': sum,\n",
    "        'predicted': predicted\n",
    "    })\n",
    "\n",
    "# sort results: allows the n lowest to be collected\n",
    "sorted_results = sorted(agg, key=lambda x: (x['difference']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best 4 models\n",
    "for model in sorted_results[:4]:\n",
    "    # get data\n",
    "    zipcode = model['zipcode']\n",
    "    data_zipcode = df_zipcode_clean[[zipcode]]\n",
    "    data_train = data_zipcode.T[train_columns].T\n",
    "    \n",
    "    # compute_arima\n",
    "    predictions = compute_arima(data_train.iloc[:,0], q=1, residuals_plot=True)\n",
    "\n",
    "    # plot predictions\n",
    "    rolling_plot(data_zipcode.T[test_columns].T, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
