{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import zipcodes\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5b7faa1db8b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m   \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m )\n\u001b[1;32m---> 22\u001b[1;33m df_labor[['CountyName', 'State']] = df[['CountyName', 'State']].apply(\n\u001b[0m\u001b[0;32m     23\u001b[0m   \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# read dataset\n",
    "with open('../data/Zip_Zhvi_SingleFamilyResidence.csv', 'rb') as f:\n",
    "    df_zillow = pd.read_csv(f).fillna(0)\n",
    "\n",
    "with open('../data/laucnty17.xlsx', 'rb') as f:\n",
    "    df_labor = pd.read_excel(f, skiprows=range(1, 6)).fillna(0)\n",
    "\n",
    "# restructure labor columnss\n",
    "df_labor = pd.DataFrame(df_labor.iloc[:, [3,9]])\n",
    "df_labor.columns = ['Location', 'Unemployment']\n",
    "df_labor[['CountyName', 'State']] = df_labor['Location'].str.split(',', expand=True)\n",
    "df_labor.drop(['Location'], axis=1, inplace=True)\n",
    "\n",
    "# convert non-timeseries\n",
    "df_zillow[['City', 'State', 'Metro', 'CountyName']] = df_zillow[['City', 'State', 'Metro', 'CountyName']].astype(str)\n",
    "df_zillow[['RegionID', 'RegionName', 'SizeRank']] = df_zillow[['RegionID', 'RegionName', 'SizeRank']].astype(int)\n",
    "\n",
    "# convert to lowercase\n",
    "df_zillow[['City', 'State', 'Metro', 'CountyName']] = df_zillow[['City', 'State', 'Metro', 'CountyName']].apply(\n",
    "  lambda x: x.astype(str).str.lower()\n",
    ")\n",
    "df_labor[['CountyName', 'State']] = df_labor[['CountyName', 'State']].apply(\n",
    "  lambda x: x.astype(str).str.lower()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes\n",
    "df = pd.merge(df_zillow, df_labor, on=['CountyName', 'State'])\n",
    "\n",
    "# remove rows with unemployment >= 4.5%\n",
    "print(df[df.Unemployment < 4.5])\n",
    "\n",
    "# remove Unemployment column\n",
    "df.drop(['Unemployment'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeseries data\n",
    "ts_start = df.columns.get_loc('1996-04') + 1\n",
    "ts_end = df.columns.get_loc('2017-09')\n",
    "date_columns = df.iloc[:, ts_start:ts_end].columns.tolist()\n",
    "\n",
    "# ensure integer timeseries\n",
    "df[date_columns] = df[date_columns].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# metro areas: these values didn't get filtered based on\n",
    "#     unemployment, and likely better placed above the\n",
    "#     corresponding logic.\n",
    "#\n",
    "hot_springs = df.loc[(df['Metro'] == 'hot springs') & (df['State'] == 'ar')]\n",
    "little_rock = df.loc[(df['Metro'] == 'little rock') & (df['State'] == 'ar')]\n",
    "fayetteville = df.loc[(df['Metro'] == 'fayetteville') & (df['State'] == 'ar')]\n",
    "searcy = df.loc[(df['Metro'] == 'searcy') & (df['State'] == 'ar')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeseries plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(hot_springs[date_columns].mean(), linestyle='solid')\n",
    "ax.plot(little_rock[date_columns].mean(), linestyle='solid')\n",
    "ax.plot(fayetteville[date_columns].mean(), linestyle='solid')\n",
    "ax.plot(searcy[date_columns].mean(), linestyle='solid')\n",
    "\n",
    "# decrease ticks\n",
    "xmin, xmax = ax.get_xlim()\n",
    "ax.set_xticks(np.round(np.linspace(xmin, xmax, 23), 2))\n",
    "\n",
    "# rotate ticks + show legend\n",
    "plt.xticks(rotation=90)\n",
    "plt.gca().legend(('hot_springs', 'little_rock', 'fayetteville', 'searcy'))\n",
    "\n",
    "# show overall plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter specific states\n",
    "df = df.loc[df['State'].isin(['md','va', 'nh', 'ma', 'dc'])]\n",
    "\n",
    "# remove specific cities\n",
    "df = df.loc[-((df['Metro'] == 'baltimore') & (df['State'] == 'md'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train: collapse column by median\n",
    "train_start = df.columns.get_loc('1997-01')\n",
    "train_stop = df.columns.get_loc('2017-01')\n",
    "test_stop = df.columns.get_loc('2017-09')\n",
    "train_columns = df.iloc[:, train_start:train_stop].columns.tolist()\n",
    "test_columns = df.iloc[:, (train_stop + 1):test_stop].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with 0's beginning (1997-01) with trainset\n",
    "date_columns = df.iloc[:, train_start:test_stop].columns.tolist()\n",
    "\n",
    "df[date_columns] = df[date_columns].replace(0, np.nan).dropna(\n",
    "    axis=0,\n",
    "    how='any',\n",
    "    subset=date_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# transpose dataframe: left column data, right column value\n",
    "#\n",
    "#     date1  val1\n",
    "#     date2  val2\n",
    "#      ...   ...\n",
    "#     daten  valn\n",
    "#\n",
    "df_train = df[train_columns].median().T\n",
    "df_test = df[test_columns].median().T\n",
    "\n",
    "#\n",
    "# build arima model:\n",
    "#\n",
    "#     AR: autoregression, uses observations from previous time steps as input to\n",
    "#         a regression equation to predict the value at the next time step.\n",
    "#\n",
    "#     I: integrated, use of differencing of raw observations, or subtracting an\n",
    "#         observation from previous time step. The goal is to attain a time\n",
    "#         series that is stationary.\n",
    "#\n",
    "#     MA: moving average, uses the dependency between an observation and a residual\n",
    "#         error from a moving average model applied to lagged observations.\n",
    "#\n",
    "model = ARIMA(df_train, order=(5,1,0))\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residual errors\n",
    "residuals = DataFrame(model_fit.resid)\n",
    "residuals.plot()\n",
    "plt.show()\n",
    "\n",
    "# plot kernel density estimation\n",
    "residuals.plot(kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive statistics on residual\n",
    "print(residuals.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# rolling prediction (verify model): month +2, since the train\n",
    "#     includes 2017-01.\n",
    "#\n",
    "# Note: rolling prediction is required since there is an implicit\n",
    "#       dependence on observations in prior time steps inheritted\n",
    "#       when autogressive (AR) model was defined.\n",
    "#\n",
    "history = [x for x in df_train]\n",
    "predictions = list()\n",
    "iterations = (12-len(df_test)) + 18\n",
    "\n",
    "for t in range(iterations):\n",
    "    model = ARIMA(history, order=(5,1,0))\n",
    "    model_fit = model.fit(disp=0)\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    predictions.append(yhat)\n",
    "\n",
    "    if t > 10:\n",
    "        year = 2018\n",
    "        month = (t+2) % 12\n",
    "        if month == 0:\n",
    "            month = 12\n",
    "    else:\n",
    "        year = 2017\n",
    "        month = t+2\n",
    "        if month == 0:\n",
    "            month = 12\n",
    "\n",
    "    print('\\n===============================================')\n",
    "    print('date: {}-{:01d}'.format(year, month))\n",
    "    print('-----------------------------------------------')\n",
    "\n",
    "    #\n",
    "    # observation: if current value doesn't exist from test, append current\n",
    "    #     predition, to ensure successive rolling prediction computed.\n",
    "    #\n",
    "    try:\n",
    "        obs = df_test[t]\n",
    "        print('predicted={:03f}, expected={:03f}'.format(float(yhat), obs))\n",
    "        print('prediction difference: {:03f}'.format(abs(1-float(yhat)/obs)))\n",
    "        print('\\n===============================================')\n",
    "        error = mean_squared_error(df_test, predictions)\n",
    "        print('Test MSE: {:03f}\\n\\n'.format(error))\n",
    "    except:\n",
    "        obs = yhat\n",
    "        print('predicted={:03f}'.format(float(yhat)))\n",
    "\n",
    "    history.append(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rolling prediction\n",
    "plt.plot(df_test)\n",
    "plt.plot(predictions, color='red')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stationarity test\n",
    "def difference(dataset, delta):\n",
    "    diff = list()\n",
    "    for i in range(1, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - delta]\n",
    "        diff.append(value)\n",
    "    return pd.Series(diff)\n",
    "\n",
    "def compute_arima(df_train):\n",
    "    #\n",
    "    # Note: rolling prediction is required since there is an implicit\n",
    "    #       dependence on observations in prior time steps inheritted\n",
    "    #       when autogressive (AR) model was defined.\n",
    "    #\n",
    "    history = [x for x in df_train]\n",
    "    predictions = list()\n",
    "    iterations = (12-len(df_test)) + 18\n",
    "\n",
    "    for t in range(iterations):\n",
    "        #\n",
    "        # determine stationarity value: no need to difference more than\n",
    "        #     half the lenght of the given series.\n",
    "        #\n",
    "        # @delta, the difference factor.\n",
    "        #\n",
    "        for delta in range(int(len(history) / 2)):\n",
    "            stationary = difference(history, delta)\n",
    "            stationary.index = history[1:]\n",
    "            result = adfuller(stationary)\n",
    "            autoreg = delta\n",
    "            print('delta: {}, p: {}'.format(delta, result[1]))\n",
    "\n",
    "            #\n",
    "            # generate model: use dickey-fuller significance test using\n",
    "            #     custom 'difference' function. Sometimes, statsmodels\n",
    "            #     arima algorithm will not agree with our pvalue, so it\n",
    "            #     is forced to search for another significant difference.\n",
    "            #\n",
    "            if (result[1] <= 0.05):\n",
    "                try:\n",
    "                    model = ARIMA(history, order=(autoreg, 2, 2))\n",
    "                    model_fit = model.fit(disp=0)\n",
    "                    break\n",
    "                except:\n",
    "                    print('stationarity not adequate')\n",
    "                    continue\n",
    "\n",
    "        output = model_fit.forecast()\n",
    "        yhat = output[0]\n",
    "\n",
    "        #\n",
    "        # observation: if current value doesn't exist from test, append current\n",
    "        #     predition, to ensure successive rolling prediction computed.\n",
    "        #\n",
    "        try:\n",
    "            obs = df_test[t]\n",
    "        except:\n",
    "            obs = yhat\n",
    "        history.append(obs)\n",
    "        \n",
    "        predictions.append(yhat)\n",
    "        print('predictions: {}'.format(predictions))\n",
    "\n",
    "    return(predictions)\n",
    "\n",
    "def get_zipcode(city, state):\n",
    "    result = zipcodes.filter_by(\n",
    "        zipcodes.list_all(),\n",
    "        active=True,\n",
    "        city=city,\n",
    "        state=state\n",
    "    )\n",
    "\n",
    "    if result and result[0] and result[0]['zip_code']:\n",
    "        return(result[0]['zip_code'])\n",
    "    else:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add zipcode column\n",
    "df['zip_code'] = df[['City', 'State']].apply(\n",
    "    lambda x: get_zipcode(\n",
    "        x['City'].upper(),\n",
    "        x['State'].upper()\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by zipcode\n",
    "df_zipcode = df.groupby('zip_code').agg(np.median).dropna().T\n",
    "\n",
    "#\n",
    "# remove columns: column 0 indicates an NaN column\n",
    "#\n",
    "df_zipcode_clean = df_zipcode.drop([\n",
    "    'RegionName',\n",
    "    'RegionID',\n",
    "    'SizeRank'\n",
    "], axis=0)\n",
    "df_zipcode_clean = df_zipcode_clean.drop([0], axis=1)\n",
    "\n",
    "df_zipcode_clean.plot(legend=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## iterate columns\n",
    "results = []\n",
    "for column in df_zipcode_clean:\n",
    "    predictions = compute_arima(df_zipcode_clean[column])\n",
    "    results.append({\n",
    "        'zip_code': df_zipcode_clean[column].name,\n",
    "        'predictions': predictions\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
